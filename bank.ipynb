{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the dataset\n",
    "data = pd.read_csv(\"churn.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Distinguish the feature and target set and divide the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Identify the features and target variable\n",
    "# X = data.drop(columns=[\"Exited\"])  # Features (exclude \"Exited\" column)\n",
    "X = pd.get_dummies(data.drop(columns=[\"Exited\", \"Surname\"]), columns=[\"Geography\", \"Gender\"])\n",
    "y = data[\"Exited\"]  # Target variable\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Meaning of this line pd.get_dummies(data.drop(columns=[\"Exited\", \"Surname\"]), columns=[\"Geography\", \"Gender\"])\n",
    "It removes the columns \"Exited\" and \"Surname\" from the dataset, effectively selecting the features you want to use for your machine learning model. The resulting DataFrame X contains only the features you will use for training and testing.\n",
    "\n",
    "One-Hot Encoding: It performs one-hot encoding on the categorical columns \"Geography\" and \"Gender.\" One-hot encoding is a technique used to convert categorical variables into a binary (0 or 1) format, which is suitable for machine learning models. Each unique category within a categorical column is transformed into a binary column, where a 1 represents the presence of that category, and 0 represents its absence.\n",
    "\n",
    "For example, if \"Geography\" has three unique values (e.g., \"France,\" \"Spain,\" \"Germany\") and \"Gender\" has two unique values (e.g., \"Male,\" \"Female\"), after applying pd.get_dummies, the resulting X DataFrame would include new columns like:\n",
    "\n",
    "\"Geography_France,\" \"Geography_Spain,\" \"Geography_Germany\"\n",
    "\"Gender_Male,\" \"Gender_Female\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Normalize the train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same scaler\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Initialize and build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "# Step 4: Build a binary classification model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Print the accuracy score and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8595\n",
      "Confusion Matrix:\n",
      " [[1557   50]\n",
      " [ 231  162]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate and print the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify the points of improvement and implement the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8595\n",
      "Confusion Matrix:\n",
      " [[1557   50]\n",
      " [ 231  162]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Generate and print the confusion matrix and classification report\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes\n",
    "A confusion matrix is a table that is used to describe the performance of a classification model on a set of test data for which the true values are known. It's particularly useful for binary classification tasks (where there are only two classes or categories).\n",
    "\n",
    "True Positives (TP): These are instances where the model correctly predicted the positive class. In a binary classification problem, it represents the number of correct \"positive\" predictions.\n",
    "\n",
    "True Negatives (TN): These are instances where the model correctly predicted the negative class. It represents the number of correct \"negative\" predictions.\n",
    "\n",
    "False Positives (FP): These are instances where the model incorrectly predicted the positive class when the true class is negative. It represents the number of \"false alarms\" or Type I errors.\n",
    "\n",
    "False Negatives (FN): These are instances where the model incorrectly predicted the negative class when the true class is positive. It represents the number of \"missed opportunities\" or Type II errors.\n",
    "\n",
    "True Positives (TP) | False Negatives (FN)\n",
    "------------------- | ----------------------\n",
    "False Positives (FP) | True Negatives (TN)\n",
    "\n",
    "\n",
    "\n",
    "The accuracy score is a simple and commonly used metric for classification models. It measures the overall correctness of the model's predictions.It represents the ratio of correct predictions to the total number of predictions made by the model. A higher accuracy score indicates that the model is making more correct predictions, while a lower accuracy score suggests that the model is making more incorrect predictions.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
