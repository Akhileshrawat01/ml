{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local minimum found at x = -2.9999960768114153, Cost = 1.5391408670843192e-11\n"
     ]
    }
   ],
   "source": [
    "# Define the cost function and its gradient\n",
    "def cost_function(x):\n",
    "    return (x + 3) ** 2\n",
    "\n",
    "def gradient(x):\n",
    "    return 2 * (x + 3)\n",
    "\n",
    "# Initialize the starting point\n",
    "x = 2\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1  # Step size\n",
    "max_iterations = 1000  # Maximum number of iterations\n",
    "epsilon = 1e-5  # Convergence threshold\n",
    "\n",
    "# Gradient Descent algorithm\n",
    "for i in range(max_iterations):\n",
    "    gradient_value = gradient(x)\n",
    "    x = x - learning_rate * gradient_value\n",
    "\n",
    "    # Check for convergence\n",
    "    if abs(gradient_value) < epsilon:\n",
    "        break\n",
    "\n",
    "print(f\"Local minimum found at x = {x}, Cost = {cost_function(x)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "Gradient Descent is an iterative optimization algorithm used to minimize a cost function in order to find the local or global minima of the function. \n",
    "\n",
    "Objective Function (Cost Function): Gradient Descent is used to minimize a cost function, also known as an objective function. This cost function typically measures the error or loss between predicted values and actual values in a machine learning model.\n",
    "\n",
    "Gradient (Derivative): The gradient is a vector that points in the direction of the steepest increase in the cost function. It provides information about how much the cost function will change with respect to small changes in the model parameters.\n",
    "This gradient function represents the rate of change of the cost function with respect to changes in the parameter \n",
    "x. In Gradient Descent, it's used to update the parameter \n",
    "x in the direction that minimizes the cost function. The negative gradient (as used in Gradient Descent) points in the direction of steepest decrease, which is why we subtract it from the current value of \n",
    "x\n",
    "x during each iteration to approach the local minimum of the cost function.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
